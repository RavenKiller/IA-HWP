{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some visualization in habitat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "+ [habitat-sim colab example](https://github.com/facebookresearch/habitat-sim/blob/main/examples/tutorials/colabs/ECCV_2020_Navigation.ipynb)\n",
    "+ [habitat official tutorials](https://aihabitat.org/tutorial/2020/)\n",
    "+ [habitat doc](https://aihabitat.org/docs/habitat-lab/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Avoid too much logs\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"GLOG_minloglevel\"] = \"2\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as utils\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "from habitat_baselines.utils.common import center_crop\n",
    "import habitat\n",
    "from habitat.utils.visualizations import maps \n",
    "\n",
    "import magnum as mn\n",
    "import math\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "def display_sample(rgb_obs, semantic_obs, depth_obs, idx=0, trajectory_id=0):\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "    rgb_img.save(\"trajectory%d_%d_rgb.png\"%(trajectory_id,idx))\n",
    "    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "    semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "    semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "    semantic_img = semantic_img.convert(\"RGBA\")\n",
    "    semantic_img.save(\"trajectory%d_%d_semantic.png\"%(trajectory_id,idx))\n",
    "    \n",
    "    depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "    depth_img.save(\"trajectory%d_%d_depth.png\"%(trajectory_id,idx))\n",
    "\n",
    "    arr = [rgb_img, semantic_img, depth_img]\n",
    "    titles = ['rgb', 'semantic', 'depth']\n",
    "    plt.figure(figsize=(12 ,8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i+1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show()\n",
    "def display_map(topdown_map, key_points=None):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(topdown_map)\n",
    "    # plot points on map\n",
    "    if key_points is not None:\n",
    "        for point in key_points:\n",
    "            plt.plot(point[0], point[1], marker=\"o\", markersize=10, alpha=0.8)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    \n",
    "    # Note: all sensors must have the same resolution\n",
    "    sensors = {\n",
    "        \"color_sensor\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.COLOR,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "        },\n",
    "        \"depth_sensor\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.DEPTH,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "        },\n",
    "        \"semantic_sensor\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.SEMANTIC,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "        },  \n",
    "    }\n",
    "    \n",
    "    sensor_specs = []\n",
    "    for sensor_uuid, sensor_params in sensors.items():\n",
    "        if settings[sensor_uuid]:\n",
    "            sensor_spec = habitat_sim.CameraSensorSpec() # SensorSpec\n",
    "            sensor_spec.uuid = sensor_uuid\n",
    "            sensor_spec.sensor_type = sensor_params[\"sensor_type\"]\n",
    "            sensor_spec.resolution = sensor_params[\"resolution\"]\n",
    "            sensor_spec.position = sensor_params[\"position\"]\n",
    "\n",
    "            sensor_specs.append(sensor_spec)\n",
    "            \n",
    "    # Here you can specify the amount of displacement in a forward action and the turn angle\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\": habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.25)\n",
    "        ),\n",
    "        \"turn_left\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=15.0)\n",
    "        ),\n",
    "        \"turn_right\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=15.0)\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "test_scene = \"/share/home/tj90055/hzt/IA-HWP/data/scene_datasets/mp3d/8WUmhLawc2A/8WUmhLawc2A.glb\"\n",
    "start_point = np.array([\n",
    "                -6.2812700271606445,\n",
    "                0.10065700113773346,\n",
    "                -0.39065900444984436\n",
    "            ])\n",
    "start_rotation = np.array([\n",
    "                0,\n",
    "                -0.7306023269338372,\n",
    "                0,\n",
    "                0.6828032219306397\n",
    "            ])\n",
    "end_point = np.array([\n",
    "                        -16.707500457763672,\n",
    "                        0.10065700113773346,\n",
    "                        -8.674070358276367\n",
    "                    ])\n",
    "sim_settings = {\n",
    "    \"width\": 512,  # Spatial resolution of the observations    \n",
    "    \"height\": 512,\n",
    "    \"scene\": test_scene,  # Scene path\n",
    "    \"default_agent\": 0,  \n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters\n",
    "    \"color_sensor\": True,  # RGB sensor\n",
    "    \"semantic_sensor\": True,  # Semantic sensor\n",
    "    \"depth_sensor\": True,  # Depth sensor\n",
    "    \"seed\": 1,\n",
    "}\n",
    "trajectory_id = 607\n",
    "cfg = make_cfg(sim_settings)\n",
    "sim = habitat_sim.Simulator(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(sim_settings[\"seed\"])\n",
    "sim.seed(sim_settings[\"seed\"])\n",
    "\n",
    "# Set agent state\n",
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent_state.position = start_point\n",
    "agent_state.rotation = start_rotation\n",
    "agent.set_state(agent_state)\n",
    "\n",
    "# Get agent state\n",
    "agent_state = agent.get_state()\n",
    "print(\"agent_state: position\", agent_state.position, \"rotation\", agent_state.rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Take random actions and display sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 0\n",
    "action_names = list(\n",
    "    cfg.agents[\n",
    "        sim_settings[\"default_agent\"]\n",
    "    ].action_space.keys()\n",
    ")\n",
    "\n",
    "max_frames = 5\n",
    "\n",
    "while total_frames < max_frames:\n",
    "    action = random.choice(action_names)\n",
    "    print(\"action\", action)\n",
    "    observations = sim.step(action)\n",
    "    rgb = observations[\"color_sensor\"]\n",
    "    print(rgb.shape)\n",
    "    semantic = observations[\"semantic_sensor\"]\n",
    "    depth = observations[\"depth_sensor\"]\n",
    "    \n",
    "    display_sample(rgb, semantic, depth)\n",
    "    \n",
    "    total_frames += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set position and save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(sim_settings[\"seed\"])\n",
    "sim.seed(sim_settings[\"seed\"])\n",
    "\n",
    "# Set agent state\n",
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent_state.position = start_point\n",
    "agent_state.rotation = start_rotation\n",
    "agent.set_state(agent_state)\n",
    "\n",
    "# Get agent state\n",
    "agent_state = agent.get_state()\n",
    "print(\"agent_state: position\", agent_state.position, \"rotation\", agent_state.rotation)\n",
    "\n",
    "# Get observations\n",
    "observations = sim.get_sensor_observations()\n",
    "rgb = observations[\"color_sensor\"]\n",
    "print(rgb.shape)\n",
    "semantic = observations[\"semantic_sensor\"]\n",
    "depth = observations[\"depth_sensor\"]\n",
    "display_sample(rgb, semantic, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display top down map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_per_pixel = 0.1\n",
    "nav_points = start_point\n",
    "top_down_map = maps.get_topdown_map(\n",
    "    sim.pathfinder, height=nav_points[1], meters_per_pixel=meters_per_pixel\n",
    ")\n",
    "recolor_map = np.array(\n",
    "    [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    ")\n",
    "top_down_map = recolor_map[top_down_map]\n",
    "print(\"\\nDisplay the map with key_point overlay:\")\n",
    "display_map(top_down_map, key_points=[nav_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display map and shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 4  # @param {type:\"integer\"}\n",
    "sim.pathfinder.seed(seed)\n",
    "start_point = np.array([-6.2812700271606445,0.10065700113773346,-0.39065900444984436])\n",
    "start_rotation = np.array([0,-0.7306023269338372,0,0.6828032219306397])\n",
    "end_point = np.array([-16.707500457763672,0.10065700113773346,-8.674070358276367])\n",
    "# fmt off\n",
    "# @markdown 1. Sample valid points on the NavMesh for agent spawn location and pathfinding goal.\n",
    "# fmt on\n",
    "sample1 = start_point\n",
    "sample2 = end_point\n",
    "\n",
    "# @markdown 2. Use ShortestPath module to compute path between samples.\n",
    "path = habitat_sim.ShortestPath()\n",
    "path.requested_start = sample1\n",
    "path.requested_end = sample2\n",
    "\n",
    "found_path = sim.pathfinder.find_path(path)\n",
    "geodesic_distance = path.geodesic_distance\n",
    "path_points = path.points\n",
    "# @markdown - Success, geodesic path length, and 3D points can be queried.\n",
    "print(\"found_path : \" + str(found_path))\n",
    "print(\"geodesic_distance : \" + str(geodesic_distance))\n",
    "print(\"path_points : \" + str(path_points))\n",
    "\n",
    "# @markdown 3. Display trajectory (if found) on a topdown map of ground floor\n",
    "if found_path:\n",
    "    meters_per_pixel = 0.025\n",
    "    scene_bb = sim.get_active_scene_graph().get_root_node().cumulative_bb\n",
    "    height = scene_bb.y().min\n",
    "    if display:\n",
    "        top_down_map = maps.get_topdown_map(\n",
    "            sim.pathfinder, height, meters_per_pixel=meters_per_pixel\n",
    "        )\n",
    "        recolor_map = np.array(\n",
    "            [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "        )\n",
    "        top_down_map = recolor_map[top_down_map]\n",
    "        grid_dimensions = (top_down_map.shape[0], top_down_map.shape[1])\n",
    "        # convert world trajectory points to maps module grid points\n",
    "        trajectory = [\n",
    "            maps.to_grid(\n",
    "                path_point[2],\n",
    "                path_point[0],\n",
    "                grid_dimensions,\n",
    "                pathfinder=sim.pathfinder,\n",
    "            )\n",
    "            for path_point in path_points\n",
    "        ]\n",
    "        grid_tangent = mn.Vector2(\n",
    "            trajectory[1][1] - trajectory[0][1], trajectory[1][0] - trajectory[0][0]\n",
    "        )\n",
    "        path_initial_tangent = grid_tangent / grid_tangent.length()\n",
    "        initial_angle = math.atan2(path_initial_tangent[0], path_initial_tangent[1])\n",
    "        # draw the agent and trajectory on the map\n",
    "        maps.draw_path(top_down_map, trajectory)\n",
    "        maps.draw_agent(\n",
    "            top_down_map, trajectory[0], initial_angle, agent_radius_px=8\n",
    "        )\n",
    "        print(\"\\nDisplay the map with agent and path overlay:\")\n",
    "        display_map(top_down_map)\n",
    "\n",
    "    # @markdown 4. (optional) Place agent and render images at trajectory points (if found).    \n",
    "    display_path_agent_renders = True  # @param{type:\"boolean\"}\n",
    "    final_view = True\n",
    "    if display_path_agent_renders:\n",
    "        print(\"Rendering observations at path points:\")\n",
    "        tangent = path_points[1] - path_points[0]\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        for ix, point in enumerate(path_points):\n",
    "            if ix < len(path_points) - 1:\n",
    "                tangent = path_points[ix + 1] - point\n",
    "                agent_state.position = point\n",
    "                tangent_orientation_matrix = mn.Matrix4.look_at(\n",
    "                    point, point + tangent, np.array([0, 1.0, 0])\n",
    "                )\n",
    "                tangent_orientation_q = mn.Quaternion.from_matrix(\n",
    "                    tangent_orientation_matrix.rotation()\n",
    "                )\n",
    "                agent_state.rotation = utils.quat_from_magnum(tangent_orientation_q)\n",
    "                if ix==0:\n",
    "                    agent_state.rotation = start_rotation\n",
    "                agent.set_state(agent_state)\n",
    "                \n",
    "                observations = sim.get_sensor_observations()\n",
    "                rgb = observations[\"color_sensor\"]\n",
    "                semantic = observations[\"semantic_sensor\"]\n",
    "                depth = observations[\"depth_sensor\"]\n",
    "\n",
    "                # if display:\n",
    "                display_sample(rgb, semantic, depth, idx=ix)\n",
    "    final_view = True\n",
    "    if final_view:\n",
    "        point = path_points[-1]\n",
    "        agent_state.position = point\n",
    "        agent_state.rotation = start_rotation\n",
    "        agent.set_state(agent_state)\n",
    "        observations = sim.get_sensor_observations()\n",
    "        rgb = observations[\"color_sensor\"]\n",
    "        semantic = observations[\"semantic_sensor\"]\n",
    "        depth = observations[\"depth_sensor\"]\n",
    "\n",
    "        # if display:\n",
    "        display_sample(rgb, semantic, depth, idx=len(path_points))\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display map and shortest path with panoramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sim.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "VIEW_NUM = 72\n",
    "SPLIT_NUM = 90 / (360/VIEW_NUM)\n",
    "OFFSET = IMAGE_SIZE*200/256/VIEW_NUM # 6\n",
    "# OFFSET = 0\n",
    "# test_scene = \"/share/home/tj90055/hzt/IA-HWP/data/scene_datasets/mp3d/zsNo4HB9uLZ/zsNo4HB9uLZ.glb\"\n",
    "test_scene =  \"/share/home/tj90055/hzt/IA-HWP/data/scene_datasets/mp3d/zsNo4HB9uLZ/zsNo4HB9uLZ.glb\"\n",
    "ROTATE_CENTER = True\n",
    "SHORTEST_PATH = False\n",
    "EPISODE_ID = 251 # 154\n",
    "VAL_SPLIT = \"val_unseen\"\n",
    "DATA_FOLDER = \"/share/home/tj90055/hzt/IA-HWP/data/datasets/R2R_VLNCE_v1-2_preprocessed\"\n",
    "\n",
    "def get_camera_orientations(sectors=12):\n",
    "    base_angle_deg = 360 / sectors\n",
    "    base_angle_rad = math.pi / (sectors / 2)\n",
    "    orient_dict = {}\n",
    "    for k in range(0,sectors):\n",
    "        orient_dict[str(base_angle_deg*k)] = [0.0, base_angle_rad*k, 0.0]\n",
    "    return orient_dict\n",
    "def make_cfg_pano(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    sensors = {}\n",
    "    orientations = get_camera_orientations(VIEW_NUM)\n",
    "    for i, o in orientations.items():\n",
    "        if settings[\"color_sensor\"]:\n",
    "            sensors[\"color_sensor_{}\".format(i)] = {\n",
    "                    \"sensor_type\": habitat_sim.SensorType.COLOR,\n",
    "                    \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "                    \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "                    \"orientation\": o,\n",
    "                    \"hfov\": 90,\n",
    "                }\n",
    "        if settings[\"depth_sensor\"]:\n",
    "            sensors[\"depth_sensor_{}\".format(i)] = {\n",
    "                    \"sensor_type\": habitat_sim.SensorType.DEPTH,\n",
    "                    \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "                    \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "                    \"orientation\": o,\n",
    "                    \"hfov\": 90,\n",
    "                }\n",
    "        if settings[\"semantic_sensor\"]:\n",
    "            sensors[\"semantic_sensor_{}\".format(i)] = {\n",
    "                    \"sensor_type\": habitat_sim.SensorType.SEMANTIC,\n",
    "                    \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "                    \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "                    \"orientation\": o,\n",
    "                    \"hfov\": 90,\n",
    "                }\n",
    "    \n",
    "    sensor_specs = []\n",
    "    for sensor_uuid, sensor_params in sensors.items():\n",
    "        sensor_spec = habitat_sim.CameraSensorSpec() # SensorSpec\n",
    "        sensor_spec.uuid = sensor_uuid\n",
    "        sensor_spec.sensor_type = sensor_params[\"sensor_type\"]\n",
    "        sensor_spec.resolution = sensor_params[\"resolution\"]\n",
    "        sensor_spec.position = sensor_params[\"position\"]\n",
    "        sensor_spec.orientation = sensor_params[\"orientation\"]\n",
    "        sensor_spec.hfov = sensor_params[\"hfov\"]\n",
    "        # sensor_spec.VFOV = sensor_params[\"VFOV\"]\n",
    "\n",
    "        sensor_specs.append(sensor_spec)\n",
    "            \n",
    "    # Here you can specify the amount of displacement in a forward action and the turn angle\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\": habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.25)\n",
    "        ),\n",
    "        \"turn_left\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=15.0)\n",
    "        ),\n",
    "        \"turn_right\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=15.0)\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    rgb_list = [\"color_sensor_{}\".format(v) for v in orientations]\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg]), rgb_list\n",
    "\n",
    "def stitch_images(images):\n",
    "    \"\"\"Not well\"\"\"\n",
    "    # Create a Stitcher object\n",
    "    stitcher = cv2.Stitcher_create(cv2.Stitcher_PANORAMA)\n",
    "    \n",
    "    # Perform the stitching process\n",
    "    status, stitched_image = stitcher.stitch(images)\n",
    "    \n",
    "    if status == cv2.Stitcher_OK:\n",
    "        print(\"Stitching completed successfully.\")\n",
    "        return stitched_image\n",
    "    else:\n",
    "        print(f\"Error during stitching: {status}\")\n",
    "        return None\n",
    "def get_pano(observations, need_depth=False, observation_size=(2048,512)):\n",
    "    pano_rgb = []\n",
    "    pano_depth = []\n",
    "    \n",
    "    rgb_frame = []\n",
    "    depth_frame = []\n",
    "    for i in range(VIEW_NUM-1,-1,-1):\n",
    "        rgb_frame.append(center_crop(observations[RGB_LIST[i]][:,:,:3],  (int(IMAGE_SIZE/SPLIT_NUM - OFFSET), IMAGE_SIZE)))\n",
    "        if need_depth:\n",
    "            depth = (observations[DEPTH_LIST[i]].squeeze() * 255).astype(np.uint8)\n",
    "            depth = np.stack([depth for _ in range(3)], axis=2)\n",
    "            depth = center_crop(depth, (int(IMAGE_SIZE/SPLIT_NUM - OFFSET), IMAGE_SIZE))\n",
    "            depth_frame.append(depth)\n",
    "    pano_rgb = np.concatenate(rgb_frame, axis=1)\n",
    "    if ROTATE_CENTER:\n",
    "        pano_rgb = np.roll(pano_rgb, pano_rgb.shape[1]//2, axis=1)\n",
    "    if need_depth:\n",
    "        pano_depth = np.concatenate(depth_frame, axis=0)\n",
    "        if ROTATE_CENTER:\n",
    "            pano_depth = np.roll(pano_depth, pano_depth.shape[1]//2, axis=1)\n",
    "    else:\n",
    "        pano_depth = None\n",
    "    return pano_rgb, pano_depth\n",
    "\n",
    "def display_sample_pano(obs, idx=0, trajectory_id=0):\n",
    "    rgb_pano, _ = get_pano(obs)\n",
    "\n",
    "    # plt.figure(figsize=(16 ,4))\n",
    "    plt.imshow(rgb_pano)\n",
    "    plt.axis(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{trajectory_id}_{idx}.png\", dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "sim_settings = {\n",
    "    \"width\": IMAGE_SIZE,  # Spatial resolution of the observations    \n",
    "    \"height\": IMAGE_SIZE,\n",
    "    \"scene\": test_scene,  # Scene path\n",
    "    \"default_agent\": 0,  \n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters\n",
    "    \"color_sensor\": True,  # RGB sensor\n",
    "    \"semantic_sensor\": False,  # Semantic sensor\n",
    "    \"depth_sensor\": False,  # Depth sensor\n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "cfg, RGB_LIST = make_cfg_pano(sim_settings)\n",
    "DEPTH_LIST = [v.replace(\"color\", \"depth\") for v in RGB_LIST]\n",
    "sim = habitat_sim.Simulator(cfg)\n",
    "\n",
    "# total_frames = 0\n",
    "# action_names = list(\n",
    "#     cfg.agents[\n",
    "#         sim_settings[\"default_agent\"]\n",
    "#     ].action_space.keys()\n",
    "# )\n",
    "# max_frames = 2\n",
    "# while total_frames < max_frames:\n",
    "#     action = random.choice(action_names)\n",
    "#     print(\"action\", action)\n",
    "#     observations = sim.step(action)\n",
    "    \n",
    "#     display_sample_pano(observations)\n",
    "    \n",
    "#     total_frames += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(sim_settings[\"seed\"])\n",
    "sim.seed(sim_settings[\"seed\"])\n",
    "sim.pathfinder.seed(sim_settings[\"seed\"])\n",
    "\n",
    "with gzip.open(os.path.join(DATA_FOLDER, VAL_SPLIT, VAL_SPLIT+\"_gt.json.gz\"), \"r\") as f:\n",
    "    gt = json.load(f)\n",
    "with gzip.open(os.path.join(DATA_FOLDER, VAL_SPLIT, VAL_SPLIT+\".json.gz\"), \"r\") as f:\n",
    "    split_data = json.load(f)\n",
    "traj = gt[str(EPISODE_ID)]\n",
    "for v in split_data[\"episodes\"]:\n",
    "    if v[\"episode_id\"] == EPISODE_ID:\n",
    "        episode = v\n",
    "        break\n",
    "\n",
    "start_point = np.array(episode[\"start_position\"])\n",
    "start_rotation = np.array(episode[\"start_rotation\"])\n",
    "end_point = np.array(episode[\"goals\"][0][\"position\"])\n",
    "# Set agent state\n",
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "\n",
    "if SHORTEST_PATH:\n",
    "    sample1 = start_point\n",
    "    sample2 = end_point\n",
    "    \n",
    "    path = habitat_sim.ShortestPath()\n",
    "    path.requested_start = sample1\n",
    "    path.requested_end = sample2\n",
    "    \n",
    "    found_path = sim.pathfinder.find_path(path)\n",
    "    geodesic_distance = path.geodesic_distance\n",
    "    path_points = path.points\n",
    "else:\n",
    "    found_path = True\n",
    "    geodesic_distance = episode[\"info\"][\"geodesic_distance\"]\n",
    "    path_points = [np.array(v) for v in traj[\"locations\"]]\n",
    "    \n",
    "print(\"found_path : \" + str(found_path))\n",
    "print(\"geodesic_distance : \" + str(geodesic_distance))\n",
    "print(\"path_points : \" + str(path_points))\n",
    "# @markdown 3. Display trajectory (if found) on a topdown map of ground floor\n",
    "if found_path:\n",
    "    meters_per_pixel = 0.025\n",
    "    scene_bb = sim.get_active_scene_graph().get_root_node().cumulative_bb\n",
    "    height = scene_bb.y().min\n",
    "    top_down_map = maps.get_topdown_map(\n",
    "        sim.pathfinder, height, meters_per_pixel=meters_per_pixel\n",
    "    )\n",
    "    recolor_map = np.array(\n",
    "        [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n",
    "    )\n",
    "    top_down_map = recolor_map[top_down_map]\n",
    "    grid_dimensions = (top_down_map.shape[0], top_down_map.shape[1])\n",
    "    # convert world trajectory points to maps module grid points\n",
    "    trajectory = [\n",
    "        maps.to_grid(\n",
    "            path_point[2],\n",
    "            path_point[0],\n",
    "            grid_dimensions,\n",
    "            pathfinder=sim.pathfinder,\n",
    "        )\n",
    "        for path_point in path_points\n",
    "    ]\n",
    "    grid_tangent = mn.Vector2(\n",
    "        trajectory[1][1] - trajectory[0][1], trajectory[1][0] - trajectory[0][0]\n",
    "    )\n",
    "    path_initial_tangent = grid_tangent / grid_tangent.length()\n",
    "    initial_angle = math.atan2(path_initial_tangent[0], path_initial_tangent[1])\n",
    "    # draw the agent and trajectory on the map\n",
    "    maps.draw_path(top_down_map, trajectory)\n",
    "    maps.draw_agent(\n",
    "        top_down_map, trajectory[0], initial_angle, agent_radius_px=8\n",
    "    )\n",
    "    print(\"\\nDisplay the map with agent and path overlay:\")\n",
    "    display_map(top_down_map)\n",
    "\n",
    "    # @markdown 4. (optional) Place agent and render images at trajectory points (if found).    \n",
    "    display_path_agent_renders = True  # @param{type:\"boolean\"}\n",
    "    final_view = True\n",
    "    if display_path_agent_renders:\n",
    "        print(\"Rendering observations at path points:\")\n",
    "        tangent = path_points[1] - path_points[0]\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        for ix, point in enumerate(path_points):\n",
    "            if ix < len(path_points) - 1:\n",
    "                tangent = path_points[ix + 1] - point\n",
    "                agent_state.position = point\n",
    "                tangent_orientation_matrix = mn.Matrix4.look_at(\n",
    "                    point, point + tangent, np.array([0, 1.0, 0])\n",
    "                )\n",
    "                tangent_orientation_q = mn.Quaternion.from_matrix(\n",
    "                    tangent_orientation_matrix.rotation()\n",
    "                )\n",
    "                agent_state.rotation = utils.quat_from_magnum(tangent_orientation_q)\n",
    "                if ix==0:\n",
    "                    agent_state.rotation = start_rotation\n",
    "                agent.set_state(agent_state)\n",
    "                \n",
    "                observations = sim.get_sensor_observations()\n",
    "\n",
    "                # if display:\n",
    "                display_sample_pano(observations, idx=ix)\n",
    "    final_view = True\n",
    "    if final_view:\n",
    "        point = path_points[-1]\n",
    "        agent_state.position = point\n",
    "        agent_state.rotation = utils.quat_from_magnum(tangent_orientation_q)\n",
    "        agent.set_state(agent_state)\n",
    "        observations = sim.get_sensor_observations()\n",
    "\n",
    "        # if display:\n",
    "        display_sample_pano(observations, idx=len(path_points))\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a split with only single episode\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "ep_id = 120\n",
    "split = \"val_seen\"\n",
    "data_folder = Path(\"/share/home/tj90055/hzt/IA-HWP/data/datasets/R2R_VLNCE_v1-2_preprocessed\")\n",
    "unseen_folder = data_folder / split\n",
    "single_folder = data_folder / \"single{}\".format(ep_id)\n",
    "single_name = \"single{}\".format(ep_id)\n",
    "os.system(\"cp -r {} {}\".format(str(unseen_folder), str(single_folder)))\n",
    "os.makedirs(single_folder, exist_ok=True)\n",
    "with gzip.open(data_folder / split / f\"{split}.json.gz\", \"r\") as f:\n",
    "    single_data = json.load(f)\n",
    "single_episode = []\n",
    "for v in single_data[\"episodes\"]:\n",
    "    if v[\"episode_id\"] == ep_id:\n",
    "        single_episode.append(v)\n",
    "single_data[\"episodes\"] = single_episode\n",
    "with gzip.open(data_folder / single_name / (single_name+\".json.gz\"), \"w\") as f:\n",
    "    f.write(json.dumps(single_data).encode(\"utf-8\"))\n",
    "unseen_gt = data_folder/split/f\"{split}_gt.json.gz\"\n",
    "single_gt = data_folder/f\"single{ep_id}\"/f\"single{ep_id}_gt.json.gz\"\n",
    "os.system(\"cp {} {}\".format(str(unseen_gt), str(single_gt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40348015225666123 /share/home/tj90055/hzt/IA-HWP/logs/eval_results/iaw_train10/stats_ckpt_ckpt_8_val_unseen.json\n",
      "0.3654946212209901 /share/home/tj90055/hzt/IA-HWP/logs/eval_results/iaw_train10/stats_ckpt_ckpt_8_val_unseen.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "best_sr = 0\n",
    "best_spl = 0\n",
    "best_sr_file = \"\"\n",
    "best_spl_file = \"\"\n",
    "folder = Path(\"/share/home/tj90055/hzt/IA-HWP/logs/eval_results\")\n",
    "for v in folder.glob(\"*/stats_ckpt_ckpt_*_val_unseen.json\"):\n",
    "    with open(v, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if data[\"success\"]>best_sr:\n",
    "        best_sr = data[\"success\"]\n",
    "        best_sr_file = str(v)\n",
    "    if data[\"spl\"]>best_spl:\n",
    "        best_spl = data[\"spl\"]\n",
    "        best_spl_file = str(v)\n",
    "print(best_sr, best_sr_file)\n",
    "print(best_spl, best_spl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4987146529562982 /share/home/tj90055/hzt/IA-HWP/logs/eval_results/iaw_train5/stats_ckpt_ckpt_6_val_seen.json\n",
      "0.4576714629915391 /share/home/tj90055/hzt/IA-HWP/logs/eval_results/iaw_train5/stats_ckpt_ckpt_6_val_seen.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "best_sr = 0\n",
    "best_spl = 0\n",
    "best_sr_file = \"\"\n",
    "best_spl_file = \"\"\n",
    "folder = Path(\"/share/home/tj90055/hzt/IA-HWP/logs/eval_results\")\n",
    "for v in folder.glob(\"*/stats_ckpt_ckpt_*_val_seen.json\"):\n",
    "    with open(v, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if data[\"success\"]>best_sr:\n",
    "        best_sr = data[\"success\"]\n",
    "        best_sr_file = str(v)\n",
    "    if data[\"spl\"]>best_spl:\n",
    "        best_spl = data[\"spl\"]\n",
    "        best_spl_file = str(v)\n",
    "print(best_sr, best_sr_file)\n",
    "print(best_spl, best_spl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
